"""Tests for the `evaluation` module.

Use 'pytest tests/test_evaluation.py::test_name' to run a single test.

Use 'make test' to run all tests.

TODO: Do not check every parameter combination for all tests.
"""

import json


def test_scores(eval_paths_fixture, config) -> None:
    """Test the evaluation scores of a dataset of zebra puzzles."""
    # Get the scores path
    scores_path = eval_paths_fixture[0]

    theme = config.language.theme
    n_objects = config.n_objects
    n_attributes = config.n_attributes
    n_red_herring_clues_evaluated = config.n_red_herring_clues_evaluated
    model = config.model
    n_puzzles = config.n_puzzles

    scores_file_path = (
        scores_path
        / f"puzzle_scores_{model}_{theme}_{n_objects}x{n_attributes}_{n_red_herring_clues_evaluated}rh_{n_puzzles}_puzzles.txt"
    )
    with open(scores_file_path, "r") as f:
        scores_str = f.read()

    # Get the number after "best permuted cell score:"
    best_permuted_cell_score_str = scores_str.split("best permuted cell score: ")[1]
    best_permuted_cell_score_str = best_permuted_cell_score_str.split("\n")[0]
    best_permuted_cell_score = float(best_permuted_cell_score_str)

    # Check that the score file is not empty and the best permuted cell score is greater than 0
    assert isinstance(scores_str, str)
    assert len(scores_str) > 0
    assert best_permuted_cell_score > 0


def test_responses(eval_paths_fixture, config) -> None:
    """Test the responses generated by the evaluation."""
    # Get the response path
    responses_path = eval_paths_fixture[1]
    responses_file_path = responses_path / "zebra_puzzle_0_response.json"

    with open(responses_file_path, "r") as f:
        response = f.read()

    # Check the dimensions of the solution
    response_dict = json.loads(response)

    assert isinstance(response_dict, dict)
    assert len(response_dict) == config.n_objects
    assert len(response_dict["object_1"]) == config.n_attributes
