"""Tests for the `module` module.

Use 'pytest tests/test_module.py::test_name' to run a single test.

Use 'make test' to run all tests.

TODO: Do not check every parameter combination for all tests.
"""

import json
import os

from zebra_puzzles.zebra_utils import round_using_std


def test_prompt(data_paths_fixture, config) -> None:
    """Test the prompt generated by build_dataset."""
    # Get the puzzle path
    puzzle_path = data_paths_fixture[0]

    # Load the first file in the puzzle directory
    puzzle_filename = os.listdir(puzzle_path)[0]
    puzzle_file_path_str = puzzle_path / puzzle_filename

    # Load a generated puzzle
    with open(puzzle_file_path_str, "r") as f:
        prompt = f.read()

    prompt_templates = config.language.prompt_templates

    # Check if the prompt is a string and has a length greater than the first template
    assert isinstance(prompt, str)
    assert len(prompt) > len(prompt_templates[0])


def test_solution(data_paths_fixture, config) -> None:
    """Test the solutions generated by build_dataset."""
    # Get the solution path
    solution_path = data_paths_fixture[1]

    # Load a generated solution
    solution_file_path = solution_path / "zebra_puzzle_0_solution.json"
    with open(solution_file_path, "r") as f:
        solution = f.read()

    # Check the dimensions of the solution
    solution_dict = json.loads(solution)
    assert isinstance(solution_dict, dict)
    assert len(solution_dict) == config.n_objects
    assert len(solution_dict["object_1"]) == config.n_attributes


def test_red_herring_files(data_paths_fixture, config) -> None:
    """Test the red herring files generated by build_dataset."""
    # Get the red herring path
    red_herring_path = data_paths_fixture[2]

    # Load a generated red herring file
    red_herring_file_path = red_herring_path / "zebra_puzzle_0_red_herrings.txt"
    with open(red_herring_file_path, "r") as f:
        red_herring_indices_str = f.read()

    # Check that the file is empty or contains comma-separated indices
    assert red_herring_indices_str == "" or isinstance(red_herring_indices_str, str)

    # If the file is not empty, check that it contains comma-separated indices
    # and that the number of indices matches n_red_herring_clues
    # and that the maximum index is greater than or equal to n_red_herring_clues
    if red_herring_indices_str != "":
        red_herring_indices_list = red_herring_indices_str.split(",")
        i_red_herrings = [int(i) for i in red_herring_indices_list]
        assert isinstance(i_red_herrings, list)
        assert len(i_red_herrings) == config.n_red_herring_clues
        assert max(i_red_herrings) >= config.n_red_herring_clues


def test_scores(eval_paths_fixture, config) -> None:
    """Test the evaluation scores of a dataset of zebra puzzles."""
    # Get the scores path
    scores_path = eval_paths_fixture[0]

    theme = config.language.theme
    n_objects = config.n_objects
    n_attributes = config.n_attributes
    n_red_herring_clues_evaluated = config.n_red_herring_clues_evaluated
    model = config.model
    n_puzzles = config.n_puzzles

    scores_file_path = (
        scores_path
        / f"puzzle_scores_{model}_{theme}_{n_objects}x{n_attributes}_{n_red_herring_clues_evaluated}rh_{n_puzzles}_puzzles.txt"
    )
    with open(scores_file_path, "r") as f:
        scores_str = f.read()

    # Get the number after "best permuted cell score:"
    best_permuted_cell_score_str = scores_str.split("best permuted cell score: ")[1]
    best_permuted_cell_score_str = best_permuted_cell_score_str.split("\n")[0]
    best_permuted_cell_score = float(best_permuted_cell_score_str)

    # Check that the score file is not empty and the best permuted cell score is greater than 0
    assert isinstance(scores_str, str)
    assert len(scores_str) > 0
    assert best_permuted_cell_score > 0


def test_responses(eval_paths_fixture, config) -> None:
    """Test the responses generated by the evaluation."""
    # Get the response path
    responses_path = eval_paths_fixture[1]
    responses_file_path = responses_path / "zebra_puzzle_0_response.json"

    with open(responses_file_path, "r") as f:
        response = f.read()

    # Check the dimensions of the solution
    response_dict = json.loads(response)

    assert isinstance(response_dict, dict)
    assert len(response_dict) == config.n_objects
    assert len(response_dict["object_1"]) == config.n_attributes


def test_heatmaps_for_current_model(plot_paths_fixture, config) -> None:
    """Test that the model in the config has folders and files saved in the plots folder."""
    # Get the plotting path
    plots_path = plot_paths_fixture[0]

    # Get the path to the model currently in the config
    model = config.model
    model_folder = plots_path / model

    # Check that the model folder exists
    assert model_folder.exists()
    assert model_folder.is_dir()

    # Check that the model folder contains a subfolder for the number of evaluated red herring clues
    n_red_herring_clues_evaluated = config.n_red_herring_clues_evaluated
    red_herring_folder = model_folder / f"{n_red_herring_clues_evaluated}rh"
    assert red_herring_folder.exists()
    assert red_herring_folder.is_dir()

    # Check that the folder contains the expected cell score file
    n_puzzles = config.n_puzzles
    cell_score_plot_filename = f"mean_cell_score_{model}_{n_red_herring_clues_evaluated}rh_{n_puzzles}_puzzles.png"
    cell_score_plot_file_path = red_herring_folder / cell_score_plot_filename

    assert cell_score_plot_file_path.exists()
    assert cell_score_plot_file_path.is_file()
    assert os.path.getsize(cell_score_plot_file_path) > 0


def test_clue_type_difficulty_plot(plot_paths_fixture, config) -> None:
    """Test that the clue type difficulty plot file exists.

    Only test this when n_puzzles > 1.
    """
    # Get the plotting path
    plots_path = plot_paths_fixture[0]
    model = config.model
    n_red_herring_clues_evaluated = config.n_red_herring_clues_evaluated
    n_puzzles = config.n_puzzles

    # Get the path to the model currently in the config
    model_folder = plots_path / model

    # Define the filename for the clue type difficulty plot
    clue_type_difficulty_plot_filename = f"clue_type_difficulties_{model}_{n_red_herring_clues_evaluated}rh_{n_puzzles}_puzzles.png"
    clue_type_difficulty_plot_file_path = (
        model_folder / clue_type_difficulty_plot_filename
    )

    if n_puzzles == 1:
        # If n_puzzles is 1, the plot is not generated
        assert not clue_type_difficulty_plot_file_path.exists()
    else:
        # Check that the clue type difficulty plot file exists
        assert clue_type_difficulty_plot_file_path.exists()
        assert clue_type_difficulty_plot_file_path.is_file()
        assert os.path.getsize(clue_type_difficulty_plot_file_path) > 0


def test_clue_type_frequency_plots(plot_paths_fixture, config) -> None:
    """Test that the clue type frequency plot files exist."""
    # Get the plotting path
    plots_path = plot_paths_fixture[0]
    n_red_herring_clues_evaluated = config.n_red_herring_clues_evaluated
    n_puzzles = config.n_puzzles

    # Define the filename for the clue type frequency plot
    clue_type_frequency_plot_filename = f"clue_type_frequencies_{n_red_herring_clues_evaluated}rh_{n_puzzles}_puzzles.png"
    clue_type_frequency_plot_file_path = (
        plots_path / "clue_type_frequencies" / clue_type_frequency_plot_filename
    )

    # Check that the clue type frequency plot file exists
    assert clue_type_frequency_plot_file_path.exists()
    assert clue_type_frequency_plot_file_path.is_file()
    assert os.path.getsize(clue_type_frequency_plot_file_path) > 0


def test_model_comparisons(plot_paths_fixture, config) -> None:
    """Test the comparisons between models in the plots folder.

    TODO: Make sure a comparison is actually done, by running two models.
    """
    # Get the list of paths to plots for each LLM model / comparison
    plots_model_paths = plot_paths_fixture[1]

    # Check that the model folders exist
    assert len(plots_model_paths) > 0

    # If multiple models are present, test the comparisons
    if len(plots_model_paths) > 1:
        # Select a folder with "vs" in the name
        comparison_folder = next((p for p in plots_model_paths if "vs" in p.name), None)

        # Check that the folder exists
        assert comparison_folder is not None
        assert comparison_folder.exists()
        assert comparison_folder.is_dir()

        # Check that the folder contains the expected cell score diff file
        n_red_herring_clues_evaluated = config.n_red_herring_clues_evaluated
        n_puzzles = config.n_puzzles

        cell_score_diff_plot_filename = f"mean_cell_score_*{comparison_folder.name}*_{n_red_herring_clues_evaluated}rh_{n_puzzles}_puzzles.png"
        cell_score_diff_plot_file_path = (
            comparison_folder / cell_score_diff_plot_filename
        )

        # Check that the file exists and is not empty
        assert cell_score_diff_plot_file_path.exists()
        assert cell_score_diff_plot_file_path.is_file()
        assert os.path.getsize(cell_score_diff_plot_file_path) > 0

        # Check that the folder contains the expected comparison txt file
        comparison_txt_filename = next(
            (
                file
                for file in comparison_folder.iterdir()
                if file.name.startswith("comparison_")
                and file.name.endswith(
                    "_{n_red_herring_clues_evaluated}rh_{n_puzzles}_puzzles.txt"
                )
            ),
            None,
        )
        assert comparison_txt_filename is not None
        assert comparison_txt_filename.exists()
        assert comparison_txt_filename.is_file()
        assert os.path.getsize(comparison_txt_filename) > 0


def test_rounding() -> None:
    """Test the rounding function."""
    # Test a value that is not rounded
    value_rounded, std_rounded = round_using_std(value=0.4, std=0.3)
    assert value_rounded == "0.4"
    assert std_rounded == "0.3"

    # Test a value and standard deviation that is rounded
    value_rounded, std_rounded = round_using_std(value=0.464, std=0.31)
    assert value_rounded == "0.5"
    assert std_rounded == "0.3"

    # Test rounding of a negative number
    value_rounded, std_rounded = round_using_std(value=-0.464, std=0.31)
    assert value_rounded == "-0.5"
    assert std_rounded == "0.3"

    # Test rounding that requires trailing zeros
    value_rounded, std_rounded = round_using_std(value=-0.460, std=0.001)
    assert value_rounded == "-0.460"
    assert std_rounded == "0.001"

    # Test default rounding when the standard deviation is unknown (0)
    value_rounded, std_rounded = round_using_std(value=0.464, std=0)
    assert value_rounded == "0.46"
    assert std_rounded == "0"

    # Test default rounding when the standard deviation is unknown and the value has a trailing zero
    value_rounded, std_rounded = round_using_std(value=0.40, std=0)
    assert value_rounded == "0.40"
    assert std_rounded == "0"

    # Test rounding of a number much smaller than the standard deviation
    value_rounded, std_rounded = round_using_std(value=0.0002, std=0.3)
    assert value_rounded == "0.0"
    assert std_rounded == "0.3"
