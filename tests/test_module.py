"""Tests for the `module` module.

Use 'pytest tests/test_module.py::test_name' to run a single test.

Use 'make test' to run all tests.
"""

import json
import os

from zebra_puzzles.evaluation import evaluate_all


# An end-to-end test of build_dataset
def test_prompt(puzzle_and_solution_paths, config) -> None:
    """Test the prompt generated by build_dataset."""
    # Generate puzzles and get the puzzle path
    puzzle_path = puzzle_and_solution_paths[0]

    # Load the first file in the puzzle directory
    puzzle_filename = os.listdir(puzzle_path)[0]
    puzzle_file_path_str = puzzle_path / puzzle_filename

    # Load a generated puzzle
    with open(puzzle_file_path_str, "r") as f:
        prompt = f.read()

    prompt_templates = config.language.prompt_templates

    # Check if the prompt is a string and has a length greater than the first template
    assert isinstance(prompt, str)
    assert len(prompt) > len(prompt_templates[0])


def test_solution(puzzle_and_solution_paths, config) -> None:
    """Test the solutions generated by build_dataset."""
    # Generate puzzles and get the solution path
    solution_path = puzzle_and_solution_paths[1]

    # Load a generated solution
    solution_file_path = solution_path / "zebra_puzzle_0_solution.json"
    with open(solution_file_path, "r") as f:
        solution = f.read()

    # Check the dimensions of the solution
    solution_dict = json.loads(solution)
    assert isinstance(solution_dict, dict)
    assert len(solution_dict) == config.n_objects
    assert len(solution_dict["object_1"]) == config.n_attributes


def test_evaluation(puzzle_and_solution_paths, config) -> None:
    """Test the evaluation of a dataset of zebra puzzles.

    Puzzles are generated by the puzzle_and_solution_paths fixture.
    """
    print("config.n_puzzles:", config.n_puzzles)

    # Evaluate the dataset
    evaluate_all(
        n_puzzles=config.n_puzzles,
        n_objects=config.n_objects,
        n_attributes=config.n_attributes,
        model=config.model,
        theme=config.language.theme,
        generate_new_responses=config.generate_new_responses,
        n_red_herring_clues=config.n_red_herring_clues,
        n_red_herring_clues_evaluated=config.n_red_herring_clues_evaluated,
        data_folder=config.data_folder,
    )

    # Load the score file
    data_folder = config.data_folder
    theme = config.language.theme
    n_objects = config.n_objects
    n_attributes = config.n_attributes
    n_red_herring_clues = config.n_red_herring_clues
    n_puzzles = config.n_puzzles
    model = config.model

    scores_path_str = f"{data_folder}/scores/{theme}/{n_objects}x{n_attributes}/{n_red_herring_clues}rh/{model}/"
    scores_path_file_str = (
        scores_path_str
        + f"puzzle_scores_{model}_{theme}_{n_objects}x{n_attributes}_{n_red_herring_clues}_rh_{n_puzzles}_puzzles.txt"
    )
    with open(scores_path_file_str, "r") as f:
        scores_str = f.read()

    # Get the number after "best permuted cell score:"
    best_permuted_cell_score_str = scores_str.split("best permuted cell score: ")[1]
    best_permuted_cell_score_str = best_permuted_cell_score_str.split("\n")[0]
    best_permuted_cell_score = float(best_permuted_cell_score_str)

    # Check that the score file is not empty and the best permuted cell score is greater than 0
    assert isinstance(scores_str, str)
    assert len(scores_str) > 0
    assert best_permuted_cell_score > 0
